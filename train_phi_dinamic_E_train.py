from google.colab import drive
import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import load_dataset
import faiss
import numpy as np
import math

# 1. –ü–û–î–ö–õ–Æ–ß–ï–ù–ò–ï GOOGLE DRIVE
drive.mount('/content/drive')
BASE_OUTPUT_DIR = "/content/drive/MyDrive/LLM_IDIAS_Experiments" 

# –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é –ø–∞–ø–∫—É, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)

# --- 2. –ù–ê–°–¢–†–û–ô–ö–ê –ü–ê–†–ê–ú–ï–¢–†–û–í –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–ê ---

MODEL_NAME = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
MAX_SEQ_LENGTH = 512
LORA_R = 8
LORA_ALPHA = 16
LORA_DROPOUT = 0.05

# üö® –ü–ê–†–ê–ú–ï–¢–†–´, –ö–û–¢–û–†–´–ï –ù–£–ñ–ù–û –ú–ï–ù–Ø–¢–¨ –î–õ–Ø –ó–ê–ü–£–°–ö–û–í: üö®
PHI_LAMBDA = 0.25 

PHI_SCALE_FACTOR = 10000.0
FAISS_DIMENSION = 256
K_NEIGHBORS = 5 # –î–æ–±–∞–≤–ª–µ–Ω –≤ –∫–ª–∞—Å—Å PhiTrainer, –Ω–æ –æ—Å—Ç–∞–≤–∏–º –∑–¥–µ—Å—å –∫–∞–∫ –Ω–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ

# –ü–£–¢–¨ –î–õ–Ø –°–û–•–†–ê–ù–ï–ù–ò–Ø 
# üü¢ –ò–ó–ú–ï–ù–ï–ù–ò–ï: –î–æ–±–∞–≤–ª—è–µ–º —Å—É—Ñ—Ñ–∏–∫—Å "_E_trainable" 
output_folder_name = f"adapters_lambda_{str(PHI_LAMBDA).replace('.', '_')}_E_trainable"
FINAL_OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR, output_folder_name)
os.makedirs(FINAL_OUTPUT_DIR, exist_ok=True)

print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {FINAL_OUTPUT_DIR}")

# --- 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ QLoRA –∏ –ú–æ–¥–µ–ª–∏ (TinyLlama-1.1B) ---

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è 4-–±–∏—Ç–Ω–æ–≥–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto"
)
model.config.use_cache = False
model = prepare_model_for_kbit_training(model)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è QLoRA –¥–ª—è Llama-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
lora_config = LoraConfig(
    r=LORA_R,
    lora_alpha=LORA_ALPHA,
    lora_dropout=LORA_DROPOUT,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
    
    # üü¢ –ö–õ–Æ–ß–ï–í–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï: –í–∫–ª—é—á–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü –≤—Ö–æ–¥–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (E) –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ (L)
    modules_to_save=["embed_tokens", "lm_head"] 
)
model = get_peft_model(model, lora_config)

# üü¢ –í–ê–ñ–ù–û: –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤–∫–ª—é—á–µ–Ω—ã –¥–ª—è –º–æ–¥—É–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –æ–±—É—á–∞—Ç—å
for name, param in model.named_parameters():
    if "embed_tokens" in name or "lm_head" in name:
        param.requires_grad = True

# –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
data = load_dataset("Abirate/english_quotes")

tokenized_data = data.map(
    lambda samples: tokenizer(samples["quote"], max_length=MAX_SEQ_LENGTH, truncation=True, padding="max_length"), 
    batched=True
)

# --- 2. Custom Trainer (–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –¥–ª—è k_neighbors) ---
class PhiTrainer(Trainer):
    # k_neighbors –¥–æ–±–∞–≤–ª–µ–Ω –≤ __init__
    def __init__(self, *args, phi_lambda=0.1, phi_scale_factor=10000.0, faiss_dimension=256, k_neighbors=5, **kwargs):
        super().__init__(*args, **kwargs)
        self.phi_lambda = phi_lambda
        self.phi_scale_factor = phi_scale_factor
        self.faiss_dimension = faiss_dimension
        self.k_neighbors = k_neighbors # –¢–µ–ø–µ—Ä—å k_neighbors –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è
        self.faiss_index = self._create_faiss_index()
        

    def _create_faiss_index(self):
        embed_layer = self.model.base_model.model.model.embed_tokens
        embedding_weights = embed_layer.weight.data.float().cpu().numpy()
        d = embedding_weights.shape[1]
        
        if d > self.faiss_dimension:
            embedding_weights = embedding_weights[:, :self.faiss_dimension]
        
        index = faiss.IndexFlatL2(self.faiss_dimension)
        index.add(embedding_weights)
        print(f"FAISS Index —Å–æ–∑–¥–∞–Ω. –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {self.faiss_dimension} (–∏—Å—Ö–æ–¥–Ω–∞—è: {d})")
        return index

    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        
        if inputs.get("labels") is None:
            inputs["labels"] = inputs["input_ids"]

        # 1. –°–¢–ê–ù–î–ê–†–¢–ù–´–ô –†–ê–°–ß–ï–¢ –ü–û–¢–ï–†–¨ (NLL)
        outputs = model(**inputs, output_hidden_states=True)
        loss_nll = outputs.loss
        
        # --- 2. –†–ê–°–ß–ï–¢ –†–ï–ì–£–õ–Ø–†–ò–ó–ê–¢–û–†–ê –ö–û–ì–ï–†–ï–ù–¢–ù–û–°–¢–ò (L_phi) ---
        last_hidden_states = outputs.hidden_states[-1] 
        
        # –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è CPU/FAISS
        H = last_hidden_states[..., :self.faiss_dimension] 
        H = H.view(-1, self.faiss_dimension).detach().cpu() 
        
        active_tokens_mask = (inputs['labels'] != -100).view(-1).cpu().numpy()
        H_numpy = H.float().numpy()
        active_states = H_numpy[active_tokens_mask]
        
        if active_states.shape[0] == 0:
            return (loss_nll, outputs) if return_outputs else loss_nll

        # –†–∞—Å—á–µ—Ç H_i (–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å/Complexity Loss)
        D, I = self.faiss_index.search(active_states, self.k_neighbors) 
        H_i = torch.tensor(np.mean(D, axis=1)).to(last_hidden_states.device)
        
        # –†–∞—Å—á–µ—Ç I_i (–ü—Ä–æ–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∞—è —Å–∏–ª–∞/Information Gain)
        I_i = 1.0 / (torch.log(H_i + 1e-6) + 1.0) 
        
        # --- 2.4. –†–ê–°–ß–ï–¢ –§–ò-–ú–ï–¢–†–ò–ö–ò –ò –†–ï–ì–£–õ–Ø–†–ò–ó–ê–¢–û–†–ê (–ü–æ—Ç–æ–∫–µ–Ω–Ω—ã–π) ---
        
        # –§–ò-–ú–ï–¢–†–ò–ö–ê (PHI_i) = I_i / H_i
        Phi_tokens = I_i / H_i 
        
        # –ù–û–í–ê–Ø –ü–û–¢–ï–†–Ø: –ö–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –ö–ê–ñ–î–û–ì–û –∑–Ω–∞—á–µ–Ω–∏—è Phi_i –æ—Ç —Ü–µ–ª–µ–≤–æ–≥–æ 1: (Phi_i - 1)^2
        L_phi_deviation_tokens = (Phi_tokens - 1.0).pow(2)
        
        # –ò—Ç–æ–≥–æ–≤–∞—è –ø–æ—Ç–µ—Ä—è L_phi: –°—Ä–µ–¥–Ω–µ–µ –ø–æ –≤—Å–µ–º –∞–∫—Ç–∏–≤–Ω—ã–º —Ç–æ–∫–µ–Ω–∞–º (–∫–∞–∫ L_NLL)
        L_phi_deviation_scalar = L_phi_deviation_tokens.mean()
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –µ–¥–∏–Ω—ã–π —Å–∫–∞–ª—è—Ä–Ω—ã–π —à—Ç—Ä–∞—Ñ
        L_phi_deviation = L_phi_deviation_scalar / self.phi_scale_factor 
        
        # -------------------------------------------------------------
        
        # 3. –ò–¢–û–ì–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø –ü–û–¢–ï–†–¨ (L_total = L_NLL + lambda * L_phi_deviation)
        loss_total = loss_nll + self.phi_lambda * L_phi_deviation
        
        # 4. –õ–û–ì–ò–†–û–í–ê–ù–ò–ï 
        if self.state.global_step % self.args.logging_steps == 0:
             self.log({
                 'loss_nll': loss_nll.item(), 
                 'loss_phi_deviation': L_phi_deviation.item(),
                 'avg_phi_metric': Phi_tokens.mean().item() 
              })
        
        return (loss_total, outputs) if return_outputs else loss_total

# --- 3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è ---
training_args = TrainingArguments(
    output_dir=FINAL_OUTPUT_DIR,
    num_train_epochs=3,
    per_device_train_batch_size=4, 
    gradient_accumulation_steps=2,
    learning_rate=2e-4,
    fp16=True, 
    logging_steps=10
)

# --- 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –ó–∞–ø—É—Å–∫ ---
train_dataset = tokenized_data["train"]

trainer = PhiTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    phi_lambda=PHI_LAMBDA,
    phi_scale_factor=PHI_SCALE_FACTOR,
    faiss_dimension=FAISS_DIMENSION,
    k_neighbors=5 # –ü–µ—Ä–µ–¥–∞–µ–º k_neighbors
)

print(f"–ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è TinyLlama —Å phi_lambda = {PHI_LAMBDA}. E-–º–∞—Ç—Ä–∏—Ü–∞ —Ç–µ–ø–µ—Ä—å –æ–±—É—á–∞–µ–º–∞.")

trainer.train()

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö LoRA-–∞–¥–∞–ø—Ç–µ—Ä–æ–≤
model.save_pretrained(FINAL_OUTPUT_DIR)
tokenizer.save_pretrained(FINAL_OUTPUT_DIR)

print(f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. LoRA-–∞–¥–∞–ø—Ç–µ—Ä—ã –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ E/L-–º–∞—Ç—Ä–∏—Ü—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {FINAL_OUTPUT_DIR}.")