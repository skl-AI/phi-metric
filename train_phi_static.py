from google.colab import drive
import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import load_dataset
import faiss
import numpy as np
import math

# 1. –ü–û–î–ö–õ–Æ–ß–ï–ù–ò–ï GOOGLE DRIVE
drive.mount('/content/drive')
BASE_OUTPUT_DIR = "/content/drive/MyDrive/LLM_IDIAS_Experiments" 

# –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é –ø–∞–ø–∫—É, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)

# --- 2. –ù–ê–°–¢–†–û–ô–ö–ê –ü–ê–†–ê–ú–ï–¢–†–û–í –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–ê ---

MODEL_NAME = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
MAX_SEQ_LENGTH = 512 # –ù–∞ T4 –º–æ–∂–Ω–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å –¥–æ 512 –∏–ª–∏ 1024
LORA_R = 8
LORA_ALPHA = 16
LORA_DROPOUT = 0.05

# üö® –ü–ê–†–ê–ú–ï–¢–†–´, –ö–û–¢–û–†–´–ï –ù–£–ñ–ù–û –ú–ï–ù–Ø–¢–¨ –î–õ–Ø –ó–ê–ü–£–°–ö–û–í: üö®
# 1. –ö–æ–Ω—Ç—Ä–æ–ª—å: 0.0
# 2. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 1: 0.1
# 3. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 2: 0.5
PHI_LAMBDA = 0.25 # <--- !!! –ú–ï–ù–Ø–¢–¨ –î–õ–Ø –ö–ê–ñ–î–û–ì–û –ó–ê–ü–£–°–ö–ê !!!

PHI_SCALE_FACTOR = 10000.0
FAISS_DIMENSION = 256 # –û—Å—Ç–∞–≤–ª—è–µ–º 256 –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ –¶–ü (—Ö–æ—Ç—è T4 —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –ª—É—á—à–µ)

# –ü–£–¢–¨ –î–õ–Ø –°–û–•–†–ê–ù–ï–ù–ò–Ø (–î–û–õ–ñ–ï–ù –ú–ï–ù–Ø–¢–¨–°–Ø –í –ó–ê–í–ò–°–ò–ú–û–°–¢–ò –û–¢ PHI_LAMBDA)
output_folder_name = f"adapters_lambda_{str(PHI_LAMBDA).replace('.', '_')}"
FINAL_OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR, output_folder_name)
os.makedirs(FINAL_OUTPUT_DIR, exist_ok=True)

print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {FINAL_OUTPUT_DIR}")

# --- 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ QLoRA –∏ –ú–æ–¥–µ–ª–∏ (TinyLlama-1.1B) ---

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è 4-–±–∏—Ç–Ω–æ–≥–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto"
)
model.config.use_cache = False
model = prepare_model_for_kbit_training(model)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è QLoRA –¥–ª—è Llama-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
lora_config = LoraConfig(
    r=LORA_R,
    lora_alpha=LORA_ALPHA,
    lora_dropout=LORA_DROPOUT,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"]
)
model = get_peft_model(model, lora_config)

# –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
data = load_dataset("Abirate/english_quotes")

tokenized_data = data.map(
    lambda samples: tokenizer(samples["quote"], max_length=MAX_SEQ_LENGTH, truncation=True, padding="max_length"), 
    batched=True
)

# --- 2. Custom Trainer ---
class PhiTrainer(Trainer):
    def __init__(self, *args, phi_lambda=0.1, phi_scale_factor=10000.0, faiss_dimension=256, **kwargs):
        super().__init__(*args, **kwargs)
        self.phi_lambda = phi_lambda
        self.phi_scale_factor = phi_scale_factor
        self.faiss_dimension = faiss_dimension
        self.faiss_index = self._create_faiss_index()
        self.k_neighbors = 5 

    def _create_faiss_index(self):
        embed_layer = self.model.base_model.model.model.embed_tokens
        embedding_weights = embed_layer.weight.data.float().cpu().numpy()
        d = embedding_weights.shape[1]
        
        if d > self.faiss_dimension:
            embedding_weights = embedding_weights[:, :self.faiss_dimension]
        
        index = faiss.IndexFlatL2(self.faiss_dimension)
        index.add(embedding_weights)
        print(f"FAISS Index —Å–æ–∑–¥–∞–Ω. –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {self.faiss_dimension} (–∏—Å—Ö–æ–¥–Ω–∞—è: {d})")
        return index

    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        
        if inputs.get("labels") is None:
            inputs["labels"] = inputs["input_ids"]

        # 1. –°–¢–ê–ù–î–ê–†–¢–ù–´–ô –†–ê–°–ß–ï–¢ –ü–û–¢–ï–†–¨ (NLL)
        outputs = model(**inputs, output_hidden_states=True)
        loss_nll = outputs.loss
        
        # --- 2. –†–ê–°–ß–ï–¢ –†–ï–ì–£–õ–Ø–†–ò–ó–ê–¢–û–†–ê –ö–û–ì–ï–†–ï–ù–¢–ù–û–°–¢–ò (L_phi) ---
        last_hidden_states = outputs.hidden_states[-1] 
        
        # –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è CPU/FAISS
        H = last_hidden_states[..., :self.faiss_dimension] 
        H = H.view(-1, self.faiss_dimension).detach().cpu() 
        
        active_tokens_mask = (inputs['labels'] != -100).view(-1).cpu().numpy()
        H_numpy = H.float().numpy()
        active_states = H_numpy[active_tokens_mask]
        
        if active_states.shape[0] == 0:
            return (loss_nll, outputs) if return_outputs else loss_nll

        # –†–∞—Å—á–µ—Ç H_i (–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å/Complexity Loss)
        D, I = self.faiss_index.search(active_states, self.k_neighbors) 
        H_i = torch.tensor(np.mean(D, axis=1)).to(last_hidden_states.device)
        
        # –†–∞—Å—á–µ—Ç I_i (–ü—Ä–æ–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∞—è —Å–∏–ª–∞/Information Gain)
        I_i = 1.0 / (torch.log(H_i + 1e-6) + 1.0) 
        
        # --- 2.4. –†–ê–°–ß–ï–¢ –§–ò-–ú–ï–¢–†–ò–ö–ò –ò –ù–û–í–û–ì–û –†–ï–ì–£–õ–Ø–†–ò–ó–ê–¢–û–†–ê –û–¢–ö–õ–û–ù–ï–ù–ò–Ø ---
        
        # –§–ò-–ú–ï–¢–†–ò–ö–ê (PHI_i) = I_i / H_i
        Phi_tokens = I_i / H_i 
        
        # –ù–û–í–ê–Ø –ü–û–¢–ï–†–Ø: –ö–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç —Ü–µ–ª–µ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è 1: (Phi_i - 1)^2
        L_phi_deviation_tokens = (Phi_tokens - 1.0).pow(2)
        
        # –£—Å—Ä–µ–¥–Ω—è–µ–º –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –Ω–æ–≤—É—é –ø–æ—Ç–µ—Ä—é
        L_phi_deviation = L_phi_deviation_tokens.mean() / self.phi_scale_factor 
        
        # -------------------------------------------------------------
        
        # 3. –ò–¢–û–ì–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø –ü–û–¢–ï–†–¨ (L_total = L_NLL + lambda * L_phi_deviation)
        loss_total = loss_nll + self.phi_lambda * L_phi_deviation
        
        # 4. –õ–û–ì–ò–†–û–í–ê–ù–ò–ï 
        if self.state.global_step % self.args.logging_steps == 0:
             self.log({
                 'loss_nll': loss_nll.item(), 
                 'loss_phi_deviation': L_phi_deviation.item(),
                 'avg_phi_metric': Phi_tokens.mean().item() 
             })
        
        return (loss_total, outputs) if return_outputs else loss_total

# --- 3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è ---
training_args = TrainingArguments(
    output_dir=FINAL_OUTPUT_DIR, # <--- –°–ö–û–†–†–ï–ö–¢–ò–†–û–í–ê–ù–û
    num_train_epochs=3,
    per_device_train_batch_size=4, # <--- –£–í–ï–õ–ò–ß–ï–ù–û –¥–ª—è T4
    gradient_accumulation_steps=2, # <--- –£–ú–ï–ù–¨–®–ï–ù–û –¥–ª—è T4 (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch_size = 8)
    learning_rate=2e-4,
    fp16=True, 
    logging_steps=10
)

# --- 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –ó–∞–ø—É—Å–∫ ---
train_dataset = tokenized_data["train"]

trainer = PhiTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    phi_lambda=PHI_LAMBDA,
    phi_scale_factor=PHI_SCALE_FACTOR,
    faiss_dimension=FAISS_DIMENSION 
)

print(f"–ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è TinyLlama —Å phi_lambda = {PHI_LAMBDA}. –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã L_phi –±—É–¥—É—Ç –≤–ª–∏—è—Ç—å –Ω–∞ LoRA-–≤–µ—Å–∞.")

trainer.train()

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö LoRA-–∞–¥–∞–ø—Ç–µ—Ä–æ–≤
model.save_pretrained(FINAL_OUTPUT_DIR) # <--- –°–ö–û–†–†–ï–ö–¢–ò–†–û–í–ê–ù–û
tokenizer.save_pretrained(FINAL_OUTPUT_DIR) # <--- –°–ö–û–†–†–ï–ö–¢–ò–†–û–í–ê–ù–û

print(f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. LoRA-–∞–¥–∞–ø—Ç–µ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {FINAL_OUTPUT_DIR}.")