import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel

# 1. –û–°–ù–û–í–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´ (–ù–ï–î–û–°–¢–ê–Æ–©–ò–ô –ë–õ–û–ö)
MODEL_NAME = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
BASE_OUTPUT_DIR = "/content/drive/MyDrive/LLM_IDIAS_Experiments" 

# –°–ø–∏—Å–æ–∫ –ø–∞–ø–æ–∫ —Å –∞–¥–∞–ø—Ç–µ—Ä–∞–º–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
EXPERIMENT_FOLDERS = [
    "adapters_lambda_0_25_seq_phi",   # –ö–æ–Ω—Ç—Ä–æ–ª—å
#    "adapters_lambda_0_1",   # –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 1
#   "adapters_lambda_0_25",  # <-- –î–û–ë–ê–í–õ–ï–ù–û: –ù–æ–≤—ã–π –∞–¥–∞–ø—Ç–µ—Ä 
#    "adapters_lambda_0_5"   # –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 2
]

# –ü—Ä–æ–º—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–ò–ó–ú–ï–ù–ï–ù–´ –ù–ê –§–û–†–ú–ê–¢ CHAT)
# TinyLlama –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —à–∞–±–ª–æ–Ω: <|system|>\n<|user|>\n{prompt}\n<|assistant|>\n
# –ú—ã –ø–æ–¥–∞–¥–∏–º –µ–π —Ç–æ–ª—å–∫–æ —á–∞—Å—Ç—å <|user|>\n{prompt}\n<|assistant|>\n
TEST_PROMPTS = [
"<|user|>\nExplain the process of photosynthesis in three main steps, starting with light absorption.\n<|assistant|>\n",
"<|user|>\nProvide a historical timeline (4 events) of the development of the internet from ARPANET to the World Wide Web.\n<|assistant|>\n",
"<|user|>\nExplain the fundamental differences between John Dewey's concept of experience and Immanuel Kant's categorical imperative. Mention their core works.\n<|assistant|>\n",
"<|user|>\nList the main differences between the philosophical schools of Stoicism and Epicureanism regarding the pursuit of happiness.\n<|assistant|>\n",
"<|user|>\nDefine the concept of 'Eigenvalue' and provide a simple, real-world example of its application in data analysis.\n<|assistant|>\n",
"<|user|>\nProvide a structured framework (pros/cons analysis, goal alignment) for making a decision between high-risk investment and stable savings.\n<|assistant|>\n",
"<|user|>\nMy team member always agrees in meetings but misses deadlines. Give me three tactical steps to resolve this sensitive conflict.\n<|assistant|>\n",
"<|user|>\nExplain the 'Theory of Constraints' and its four main steps for improving a system.\n<|assistant|>\n",
"<|user|>\nDesign a five-stage maturity model for an AI-adoption strategy in a large enterprise, focusing on Governance and Data Infrastructure.\n<|assistant|>\n",
"<|user|>\nProvide a SWOT analysis (Strengths, Weaknesses, Opportunities, Threats) for an early-stage startup entering a competitive market.\n<|assistant|>\n",
"<|user|>\nIn a world where thoughts were indeed geometrical shapes, describe what a moment of profound insight would look look like.\n<|assistant|>\n",
"<|user|>\nDescribe the concept of emergence in complex systems using the analogy of a symphony orchestra or a flock of birds.\n<|assistant|>\n",
"<|user|>\nAssume the persona of a famous minimalist writer and describe the process of reducing ideas to their core essence.\n<|assistant|>\n",
"<|user|>\nExplain the meaning of the word 'saudade' (a deep emotional state) using only abstract metaphors, not direct definitions.\n<|assistant|>\n",
"<|user|>\nWhat is the philosophical difference between 'knowledge' and 'understanding', expressed as a visual paradox?\n<|assistant|>\n",

]

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–ú–û–î–ò–§–ò–¶–ò–†–û–í–ê–ù–´ –î–õ–Ø –£–°–ü–ï–®–ù–û–ô –ì–ï–ù–ï–†–ê–¶–ò–ò)
GENERATION_KWARGS = {
    "max_new_tokens": 150,    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º
    "do_sample": True,
    "temperature": 0.3,       # –°–Ω–∏–∂–∞–µ–º
    "top_k": 50
}

# 2. –ó–ê–ì–†–£–ó–ö–ê –ë–ê–ó–û–í–û–ô –ú–û–î–ï–õ–ò –ò –¢–û–ö–ï–ù–ê–ô–ó–ï–†–ê (–ú–û–î–ò–§–ò–¶–ò–†–û–í–ê–ù–û –î–õ–Ø CPU)
print("–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token

# –í–ê–ñ–ù–û: –ú—ã –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º bnb_config –¥–ª—è CPU, –Ω–æ –Ω–∞–º –Ω—É–∂–Ω–æ –µ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
# —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–∫–∏ NameError –≤ –±–ª–æ–∫–µ 4 (–¶–∏–∫–ª —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)
class DummyBitsAndBytesConfig:
    pass
if 'BitsAndBytesConfig' not in globals():
    BitsAndBytesConfig = DummyBitsAndBytesConfig

base_model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    # quantization_config=bnb_config, # –ó–ê–ö–û–ú–ú–ï–ù–¢–ò–†–û–í–ê–ù–û
    # device_map="auto" 
)

# –ü–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ CPU, —Ç–∞–∫ –∫–∞–∫ GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
base_model.to("cpu")

# 3. –§–£–ù–ö–¶–ò–Ø –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø (–ú–û–î–ò–§–ò–¶–ò–†–û–í–ê–ù–û –î–õ–Ø CPU)
def run_generation(model, tokenizer, prompt, kwargs):
    # –¢–µ–ø–µ—Ä—å —Ç–µ–Ω–∑–æ—Ä—ã –æ—Ç–ø—Ä–∞–≤–ª—è—é—Ç—Å—è –Ω–∞ CPU
    inputs = tokenizer(prompt, return_tensors="pt").to("cpu") 
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
    with torch.no_grad():
        output_ids = model.generate(**inputs, **kwargs)
        
    # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return output_text[len(prompt):].strip()


# 4. –¶–ò–ö–õ –°–†–ê–í–ù–ï–ù–ò–Ø
print("\n--- –ù–ê–ß–ê–õ–û –°–†–ê–í–ù–ï–ù–ò–Ø ---")
for folder_name in EXPERIMENT_FOLDERS:
    adapter_path = os.path.join(BASE_OUTPUT_DIR, folder_name)
    
    # üü¢ –ò–ó–ú–ï–ù–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê: –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –ª—è–º–±–¥–∞ –¥–ª—è –≤—ã–≤–æ–¥–∞
    if "seq_phi" in folder_name:
        # –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç: adapters_lambda_X_Y_seq_phi
        parts = folder_name.split('_')
        # –°–æ–±–∏—Ä–∞–µ–º –ª—è–º–±–¥—É –∏ –¥–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∫—É
        lambda_value = parts[2] + '.' + parts[3] + ' (SEQ PHI)'
    else:
        # –°—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç: adapters_lambda_X_Y
        lambda_value = folder_name.split('_')[-2] + '.' + folder_name.split('_')[-1]
    
    if not os.path.exists(adapter_path):
        print(f"‚ö†Ô∏è –ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {folder_name}. –ü—Ä–æ–ø—É—Å–∫–∞—é.")
        continue
        
    print(f"\n################################################################")
    print(f"## –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï: LAMBDA = {lambda_value}")
    print(f"################################################################")

    # –ó–∞–≥—Ä—É–∑–∫–∞ PEFT-–∞–¥–∞–ø—Ç–µ—Ä–∞
    model = PeftModel.from_pretrained(base_model, adapter_path)
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–æ–∫
    model = model.merge_and_unload() 
    model.eval()

    for i, prompt in enumerate(TEST_PROMPTS):
        print(f"\n[–ü—Ä–æ–º—Ç {i+1}]: {prompt}")
        
        # –ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        response = run_generation(model, tokenizer, prompt, GENERATION_KWARGS)
        
        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        print(f"Response ({lambda_value}) >> {response}")

    # –û—á–∏—Å—Ç–∫–∞ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏
    del model
    # torch.cuda.empty_cache() # –ù–µ –Ω—É–∂–Ω–æ, —Ç–∞–∫ –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ–º –Ω–∞ CPU
    # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –Ω–∞ CPU
    base_model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        # quantization_config=bnb_config, # –ò—Å–∫–ª—é—á–µ–Ω–æ
        # device_map="auto" # –ò—Å–∫–ª—é—á–µ–Ω–æ
    ).to("cpu")

print("\n--- –°–†–ê–í–ù–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û ---")